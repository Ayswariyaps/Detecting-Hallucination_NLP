# Detecting-Hallucination_NLP
Detects hallucinations in LLM-generated summaries using questionâ€“answer consistency. Instead of relying only on ROUGE, this project checks whether summaries can correctly answer factual questions derived from the source document.
